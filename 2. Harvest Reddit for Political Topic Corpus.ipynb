{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "https://praw.readthedocs.org/en/v2.1.16/pages/getting_started.html\n",
      "\n",
      "Use this tutorial to scrape the r/politics and r/Conservative pages"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Building a Better Topic Model by Scraping Reddit\n",
      "\n",
      "Here we are going to take advantage of the various partisan communities on the link aggregation site Reddit.com.  Reddit has various segments, or \"Subreddits\" of the site which cater to different groups.  \n",
      "\n",
      "Here, we are going to scrape the left leaning and right leaning subreddits, and we will assume that all comments from each are 'DEM' and 'REP', respectively.  This is of course prone to some error, but if we can get the volume of the corpus high enough, it shouldn't matter.  \n",
      "\n",
      "The idea is that we want to make our topic model utilize the most relevant terms for the task at hand.  We will do this by building a dictionary from the politically oriented comments, which will then be used by Gensim when it builds our topic model using Latent Dirichlet Allocation.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import praw\n",
      "import time\n",
      "\n",
      "## Comment out the new list declaration.  I'll run this once every hour or so to build up a corpus of text with political labels\n",
      "#comments_list = []\n",
      "\n",
      "r = praw.Reddit('Comment parser example by u/Quietlike')\n",
      "poli_subreddit_list = ['Democrats', 'AllTheLeft', 'politics',\n",
      "                       'Conservative', 'Republican', \n",
      "                       'Progressive', 'Libertarian', \n",
      "                       'teaparty', 'liberal','christian']\n",
      "\n",
      "#First let's get our political comments corpus of 175 comments from various politically oriented sections of the \n",
      "#link aggregation and discussion website Reddit.com  \n",
      "counter = 0\n",
      "while counter < 10: \n",
      "    for i in poli_subreddit_list:\n",
      "        political_subreddit = r.get_subreddit(i)\n",
      "        poli_comments = r.get_comments(political_subreddit)\n",
      "    \n",
      "        for comment in poli_comments:\n",
      "            comment_item = {}\n",
      "            comment_item['comment'] = comment.body\n",
      "            comment_item['type'] = 'POLITICAL'\n",
      "            \n",
      "            if (i == 'Democrats')|(i == 'Progressive')|(i == 'liberal')|(i == 'AllTheLeft')|(i == 'politics'):\n",
      "                comment_item['partisan_affiliation'] = 'DEM'\n",
      "            else: \n",
      "                comment_item['partisan_affiliation'] = 'REP'\n",
      "            comments_list.append(comment_item)\n",
      "    time.sleep(300)\n",
      "    counter += 1\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 171
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "non_poli_subreddit_list = ['fitness', 'funny', 'pics', 'sports', 'todayilearned', 'music', 'movies','nba', 'collegebasketball']\n",
      "\n",
      "for i in non_poli_subreddit_list:\n",
      "    non_poli_subreddit = r.get_subreddit(i)\n",
      "    non_poli_comments = r.get_comments(non_poli_subreddit)\n",
      "    \n",
      "    for comment in non_poli_comments:\n",
      "        comment_item = {}\n",
      "        comment_item['comment'] = comment.body\n",
      "        comment_item['type'] = 'NOT'\n",
      "        comment_item['partisan_affiliation'] = 'NA'\n",
      "        comments_list.append(comment_item)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 170
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/johnkabler/anaconda/lib/python2.7/site-packages/pytz/__init__.py:35: UserWarning: Module argparse was already imported from /Users/johnkabler/anaconda/python.app/Contents/lib/python2.7/argparse.pyc, but /Users/johnkabler/anaconda/lib/python2.7/site-packages is being added to sys.path\n",
        "  from pkg_resources import resource_stream\n"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comment_corpus = pd.DataFrame(comments_list)\n",
      "\n",
      "comment_corpus = comment_corpus.drop_duplicates()\n",
      "\n",
      "political = comment_corpus.type == 'POLITICAL'\n",
      "political_comments = comment_corpus[political]\n",
      "political_comments.to_pickle('data/reddit_poli_comments.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 172
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Next Step:  Separating Political Comments from Non-Political Comments\n",
      "\n",
      "Now we will write the comment_corpus file, which contains over 2700 comments (approximately 900 of which are political in nature) out to a Python pickle file.  These will be processed in a separate notebook, where we will attempt to construct a classifier which successfully identifies political comments.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comment_corpus.to_pickle('data/comment_corpus.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 173
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}